{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITMAL Exercise\n",
    "\n",
    "\n",
    "REVISIONS||\n",
    "---------||\n",
    "2018-1219| CEF, initial.                  \n",
    "2018-0207| CEF, updated.           \n",
    "2018-0207| CEF, rewritten accuracy paradox section. \n",
    "2018-0305| CEF, updated with SHN comments.\n",
    "\n",
    "NOTE: checkup on (y_true,y_pred)/(y_pred,y_true) parameters and make consistent (partially done but not tested)!\n",
    "NOTE: remove SHN comment in Qa\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "There are a number of frequently uses metrics in ML, namely accuracy, precision, recall and the $F_1$ score. All are called _metrics_ (though they are not true norms, like ${\\cal L}_2$ or ${\\cal L}_1$ we saw last time).\n",
    "\n",
    "Maybe performance _score_ would be a better name than performance metric, at least for the accuracy, precision, recall we will be looking at---emphasising the conceptual distinction between the  _score-function_ and _cost(/loss/error/objective)-function_ (the later is typically a true distance/norm function).  \n",
    "\n",
    "\n",
    "You can find a lot of details on say precision and recall in Wikipedia\n",
    "\n",
    ">  https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "\n",
    "### Nomenclature\n",
    "\n",
    "| NAME | SYMBOL | ALIAS |\n",
    "| :---: | :---: | :---: |\n",
    "|true positives | $TP$ | |\n",
    "|true negatives | $TN$ | |\n",
    "|false positives| $FP$ | type I error| \n",
    "|false negatives| $FN$ | type II error |\n",
    "\n",
    "and $N = N_P + N_N$ being the total number of samples and the number of positive and negative samples\n",
    "respectively.\n",
    "\n",
    "### Precision\n",
    "\n",
    "$$\n",
    "\\def\\ba{\\begin{array}{lll}}\n",
    "\\def\\ea{\\end{array}}\n",
    "\\newcommand{\\rem}[1]{}\n",
    "\\newcommand{\\subtext}[1]{_{\\scriptsize{\\mbox{#1}}}}\n",
    "\\newcommand{\\st}[1]{\\subtext{#1}}\n",
    "\\ba\n",
    " p &= \\frac{TP}{TP + FP}\n",
    "\\ea\n",
    "$$\n",
    "\n",
    "### Recall or Sensitivity\n",
    "\n",
    "$$\n",
    "  \\ba\n",
    "    r &= \\frac{TP}{TP + FN}\\\\\n",
    "      &= \\frac{TP}{N_P}\n",
    "  \\ea\n",
    "$$\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "$$\n",
    "  \\ba\n",
    "      a &= \\frac{TP + TN}{TP + TN + FP + FN}\\\\\n",
    "        &= \\frac{TP + TN}{N}\\\\\n",
    "        &= \\frac{TP + TN}{N_P + N_N} \n",
    "  \\ea\n",
    "$$\n",
    "\n",
    "#### Accuracy Paradox\n",
    "\n",
    "A static constant model, say $p\\st{cancer}=0$ may have higher accuracy than a real model with predictive power. This is odd!\n",
    "\n",
    "Asymmetric weight could also be associated with the false positive and false negative predictions, yielding either FP of FN much more expensive than the other. Say, it is more expensive not to treat a person with cancer, than treating a person without cancer. \n",
    "\n",
    "### F-score\n",
    "\n",
    "General $\\beta$-harmonic mean of the precision and recall \n",
    "$$\n",
    "    F_\\beta = (1+\\beta^2) \\frac{2pr}{\\beta^2 p+r}\\\\\n",
    "$$ \n",
    "that for say $\\beta=2$ or $\\beta=0.5$ shifts or skews the emphasis on the two variables in the equation. Normally only the $\\beta=1$ harmonic mean is used\n",
    "\n",
    "$$\n",
    "  \\ba\n",
    "    F_1     &= \\frac{2pr}{p+r}\\\\\n",
    "            &=\\frac{2}{1/p + 1/r}\n",
    "  \\ea\n",
    "$$\n",
    "with $F$ typically being synonymous with $F_1$. \n",
    "\n",
    "If needed, find more info on Wikipedia\n",
    "\n",
    "> https://en.wikipedia.org/wiki/F1_score\n",
    "\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "For statistical classification, the confusion matrix or error matrix (or\n",
    "matching matrix in unsupervised learning) is for a two-class problem given by\n",
    "the $2\\times2$ matrix with dimensions 'actual' and 'predicted'\n",
    "\n",
    "$$   \n",
    "{\\bf M}\\st{confusion} = \n",
    "\\begin{array}{l|ll}\n",
    "                           & \\mbox{actual true} & \\mbox{actual false} \\\\ \\hline\n",
    "    \\mbox{predicted true}  & TP & FP \\\\     \n",
    "    \\mbox{predicted false} & FN & TN \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The diagonal, in the square matrix, represent predicted values being the same\n",
    "as the actual values, off-diagonal elements represent erroneous prediction.\n",
    "\n",
    "For N-class classification the matrix gives a matrix with $N$ actual\n",
    "classes and $N$ predicted classes\n",
    "\n",
    "$$\n",
    "{\\bf M}\\st{confusion} =\n",
    "  \\left[\n",
    "  \\begin{array}{llll}\n",
    "       c_{11} & c_{12} & \\cdots & c_{1n} \\\\ \n",
    "       c_{21} & c_{22} & \\cdots & c_{2n} \\\\\n",
    "       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "       c_{n1} & c_{n2} & \\cdots & c_{nn} \\\\ \n",
    "  \\end{array}\n",
    "  \\right]\n",
    "$$\n",
    "with say element $c_{21}$ being the number of actual classes '1' being predicted (erroneously) as class '2'.\n",
    "\n",
    "### Nomenclature for the Confusion Matrix\n",
    "\n",
    "The naming of the elements in the confusion matrix can be rather exotic, like _false omission rate_ (see the figure below), but we won't get to such detail here...let us stick with TP, TN, FP, FN and $F_1$!\n",
    "\n",
    "<img src=\"Figs/performance_metrics.png\" style=\"width:900px\">\n",
    "\n",
    "If you need more info on the confusion matrix:\n",
    "\n",
    ">  https://en.wikipedia.org/wiki/Confusion_matrix\n",
    "\n",
    "#### Qa Implement the Accuracy function and test it on the MNIST data.\n",
    "\n",
    "Implement a general accuracy function `MyAccuracy`, that takes `y_pred` and `y_true` as input parameters.\n",
    "\n",
    "Reuse your MNIST data loader and test the `MyAccuracy` function  both on your dummy classifier and on the Stochastic Gradient Descent classifier (with setup parameters as in [HOLM])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for:  SGD Classifier\n",
      "my a          = 0.9534857142857143\n",
      "scikit-learn a= 0.9534857142857143\n",
      "\n",
      "Accuracy for:  Dummy Classifier\n",
      "my a          = 0.9138285714285714\n",
      "scikit-learn a= 0.9138285714285714\n"
     ]
    }
   ],
   "source": [
    "# TODO: Qa...\n",
    "import sys,os\n",
    "sys.path.append(os.path.abspath('')+'/..')\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.base import BaseEstimator\n",
    "from libitmal import dataloaders as dl\n",
    "import numpy as np\n",
    "from libitmal import utils\n",
    "\n",
    "class DummyClassifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "def PredictedCondition(y_pred, y_true):\n",
    "    assert len(y_pred) == len(y_true)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for i in range(0, len(y_pred)):\n",
    "        y_pred_value = y_pred[i]\n",
    "        y_true_value = y_true[i]\n",
    "        if y_pred_value == True and y_true_value == True:\n",
    "            tp += 1\n",
    "        elif y_pred_value == False and y_true_value == True:\n",
    "            fn += 1\n",
    "        elif y_pred_value == False and y_true_value == False:\n",
    "            tn += 1\n",
    "        elif y_pred_value == True and y_true_value == False:\n",
    "            fp += 1\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def MyAccuracy(y_pred, y_true):\n",
    "    # TODO: you impl here\n",
    "    (tp, tn, fp, fn) = PredictedCondition(y_pred, y_true)\n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "\n",
    "# TEST FUNCTION: compare with Scikit-learn accuracy_score\n",
    "def TestAccuracy(name, y_true, y_pred):\n",
    "    a0=MyAccuracy(y_true, y_pred)\n",
    "    a1=accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(\"\\nAccuracy for: \", name)\n",
    "    print(\"my a          =\",a0)\n",
    "    print(\"scikit-learn a=\",a1)\n",
    "\n",
    "    utils.InRange(a0,a1)\n",
    "\n",
    "target = 5.0\n",
    "X, y = dl.MNIST_GetDataSet()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "y_train_5 = (y_train == target)\n",
    "y_test_5 = (y_test == target)\n",
    "sgd_clf = SGDClassifier(max_iter=5, tol=None, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "dummy = DummyClassifier()\n",
    "y_pred_true = cross_val_predict(sgd_clf, X_test, y_test_5, cv=3)\n",
    "y_dummy_pred_true = cross_val_predict(dummy, X_test, y_test_5, cv=3)\n",
    "TestAccuracy(\"SGD Classifier\", y_test_5, y_pred_true)\n",
    "TestAccuracy(\"Dummy Classifier\", y_test_5, y_dummy_pred_true)\n",
    "\n",
    "# SHN: Programmet dummy_classifier_Qa_Qb_2019-xx-xx_shn.py køres først. Dernæst ovenstående. Resultatet stemmer:#\n",
    "#\n",
    "#    SGD classifier\n",
    "#    my a          = 0.96\n",
    "#    scikit-learn a= 0.96\n",
    "#    DummyClassifier\n",
    "#    my a          = 0.90695\n",
    "#    scikit-learn a= 0.90695"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qb Implement Precision, Recall and $F_1$-score and test it on the MNIST data.\n",
    "\n",
    "Now, implement the `MyPrecision`, `MyRecall` and `MyF1Score` functions, again taking MNIST as input, using the SGD and the Dummy classifiers and make some test vectors to compare to the functions found in Scikit-learn..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "my precision           = 0.7496402877697842\n",
      "scikit-learn precision = 0.6909814323607427\n",
      "\n",
      "my recall           = 0.6909814323607427\n",
      "scikit-learn recall = 0.7496402877697842\n",
      "\n",
      "my f1 score           = 0.7191166321601105\n",
      "scikit-learn f1 score = 0.7191166321601105\n",
      "(0, 15992, 1508, 0)\n",
      "\n",
      "my precision           = 0\n",
      "scikit-learn precision = 0.0\n",
      "\n",
      "my recall           = 0.0\n",
      "scikit-learn recall = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "my f1 score           = 0\n",
      "scikit-learn f1 score = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def MyPrecision(y_true, y_pred):\n",
    "    # TODO: you impl here\n",
    "    (tp, tn, fp, fn) = PredictedCondition(y_pred, y_true)\n",
    "    if tp == 0 and fp == 0:\n",
    "        return 0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def MyRecall(y_true, y_pred):\n",
    "    # TODO: you impl here\n",
    "    (tp, tn, fp, fn) = PredictedCondition(y_pred, y_true)\n",
    "    if tp == 0 and fn == 0:\n",
    "        return 0\n",
    "    return tp / (tp + fn)\n",
    "    \n",
    "def MyF1Score(y_true, y_pred):\n",
    "    # TODO: you impl here\n",
    "    p = MyPrecision(y_pred, y_true)\n",
    "    r = MyRecall(y_pred, y_true)\n",
    "    if p == 0 or r == 0:\n",
    "        return 0\n",
    "    return (2* p * r)/(p + r)\n",
    "\n",
    "# TODO: your test code here!\n",
    "def TestPrecision(y_pred, y_true):\n",
    "    a0=MyPrecision(y_pred, y_true)\n",
    "    a1=precision_score(y_true, y_pred)\n",
    "    print(\"\\nmy precision           =\",a0)\n",
    "    print(\"scikit-learn precision =\",a1)\n",
    "    utils.InRange(a0,a1)\n",
    "\n",
    "def TestRecall(y_pred, y_true):\n",
    "    a0=MyRecall(y_pred, y_true)\n",
    "    a1=recall_score(y_true, y_pred)\n",
    "    print(\"\\nmy recall           =\",a0)\n",
    "    print(\"scikit-learn recall =\",a1)\n",
    "    utils.InRange(a0,a1)\n",
    "\n",
    "def TestF1(y_pred, y_true):\n",
    "    a0=MyF1Score(y_pred, y_true)\n",
    "    a1=f1_score(y_true, y_pred)\n",
    "    print(\"\\nmy f1 score           =\",a0)\n",
    "    print(\"scikit-learn f1 score =\",a1)\n",
    "    utils.InRange(a0,a1)\n",
    "\n",
    "TestPrecision(y_test_5, y_pred_true)\n",
    "TestRecall(y_test_5, y_pred_true)\n",
    "TestF1(y_test_5, y_pred_true)\n",
    "print(PredictedCondition(y_test_5, y_dummy_pred_true))\n",
    "TestPrecision(y_test_5, y_dummy_pred_true)\n",
    "TestRecall(y_test_5, y_dummy_pred_true)\n",
    "TestF1(y_test_5, y_dummy_pred_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qc The Confusion Matrix\n",
    "\n",
    "Revisit your solution to Qb in the `dummy_classifier.ipynb`. Did you manage to print the confusion matrix for both the Dummy and the SGD classifier?\n",
    "\n",
    "I got the two confusion matrices\n",
    "\n",
    "```\n",
    "M_dummy=[[18166     0]\n",
    "        [ 1834     0]]\n",
    "   \n",
    "M_SDG=[[17618   548]\n",
    "      [  267  1567]]\n",
    "\n",
    "```\n",
    "your data may look similar (but not 100% equal). See if you can print the confusion matrix (some test code below for inspiration).\n",
    "\n",
    "How are the Scikit-learn confusion matrix organized, where are the TP, FP, FN and TN located in the matrix indices, and what happens if you mess up the parameters calling\n",
    "\n",
    "```python\n",
    "confusion_matrix(y_train_pred, y_train_5)\n",
    "```\n",
    "\n",
    "instead of \n",
    "```python\n",
    "confusion_matrix(y_train_5, y_train_pred)\n",
    "```\n",
    "\n",
    "Finally, compare the real and symmetric auto-covariance matrix, $\\Sigma$, with the real but non-symmetric confusion matrix, $\\mathbf{M}$. What does the diagonal represent in the covar- and confusion matrix respectively, and why is the covar- symmetric, but the confusion not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=[[15644   466]\n",
      "   [  348  1042]]\n",
      "M=[[15992  1508]\n",
      "   [    0     0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from libitmal import utils\n",
    "import numpy as np\n",
    "\n",
    "# TEST CODE: some demo code to produce a 'test' confusion matrix using the SGD model \n",
    "#M=confusion_matrix(y_test_5, sgd_y_test_pred)\n",
    "#itmalutils.PrintMatrix(M,\"M=\")\n",
    "\n",
    "M_SGD = confusion_matrix(y_pred_true, y_test_5)\n",
    "M_Dummy = confusion_matrix(y_dummy_pred_true, y_test_5)\n",
    "utils.PrintMatrix(M_SGD, \"M=\")\n",
    "utils.PrintMatrix(M_Dummy, \"M=\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qd A Confusion Matrix Heat-map\n",
    "\n",
    "Generate a _heat map_ image for the confusion matrices, `M_dummy` and `M_SGD` respectively, getting inspiration from [HOML], pp96-97.\n",
    "\n",
    "This heat map could be an important guide for you when analysing multiclass data in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19e01028710>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(M_SGD, cmap=plt.cm.Blues)\n",
    "plt.matshow(M_Dummy, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Qe Run a classifier on your data\n",
    "\n",
    "Finally, try to run a classifier on the data-set you selected previously, perhaps starting with the SGD.\n",
    "\n",
    "Is it possible to classify at all on your data, or do we need regression instead?\n",
    "\n",
    "Are you able to do supervised learning, or are there no obvious `y_true` data in your set at all?\n",
    "\n",
    "If your data is in the form, where you are able to do supervised-classification, could you produce a confusion matrix heatmap, then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(941,)\n",
      "(941,)\n",
      "(705,)\n",
      "(705,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[28.4 24.8 26.6 31.9 20.5 25.6 25.6 22.4 28.7 26.9 26.9 35.7 22.  30.5\n 24.9 26.1 22.9 32.8 32.7 26.9 26.1 34.5 27.4 27.6 24.4 28.5 29.2 25.\n 27.9 32.4 20.2 29.5 29.1 31.  26.4 33.5 26.6 27.7 17.7 25.5 32.  29.\n 26.9 25.4 19.6 30.  31.2 36.5 30.4 29.  26.3 25.7 29.2 29.  30.7 25.\n 28.5 29.7 26.3 27.6 25.4 29.4 27.4 22.3 23.9 19.2 26.2 21.  28.2 32.7\n 23.3 30.5 27.7 35.4 31.3 25.1 25.1 22.8 27.6 24.8 27.2 30.8 23.3 17.5\n 34.8 28.5 21.3 35.8 30.4 27.2 23.3 26.3 29.3 27.6 30.8 27.2 26.2 34.\n 29.4 25.6 21.8 22.6 21.6 22.9 27.2 23.8 24.6 20.2 24.3 16.1 23.3 26.3\n 19.2 27.2 23.5 35.8 34.  35.6 17.9 30.  21.7 34.  28.6 27.4 27.4 25.8\n 24.6 26.1 23.9 19.8 32.2 20.6 29.6 27.8 31.6 29.  18.1 24.1 18.4 19.3\n 32.7 19.9 30.  28.8 26.2 31.4 34.  26.2 24.1 20.4 28.7 26.4 20.6 24.5\n 26.5 29.3 31.5 25.9 24.4 19.9 20.5 27.9 30.5 28.7 26.2 30.6 26.4 24.\n 32.5 28.2 28.4 34.8 34.2 23.9 26.6 29.5 28.4 35.6 24.9 28.5 28.8 31.6\n 32.8 18.9 29.4 25.3 18.  21.8 21.5 22.8 26.  27.1 25.8 24.9 25.6 27.7\n 28.7 28.  29.  26.1 26.9 31.8 29.3 21.9 27.  21.2 33.3 35.5 27.2 27.3\n 28.  25.9 22.1 27.5 27.1 30.3 26.8 16.1 22.9 23.8 28.  31.  28.7 29.\n 25.8 29.8 27.6 19.2 26.7 24.8 23.2 32.  22.9 29.2 23.7 26.7 27.1 26.1\n 24.7 21.4 35.3 30.8 25.5 30.7 20.  27.9 23.4 29.5 25.7 33.7 22.4 28.1\n 30.3 28.3 17.4 18.7 18.4 22.1 24.4 23.7 27.4 29.6 22.5 18.  32.8 24.8\n 27.3 29.3 24.9 33.5].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-746b6f73a685>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0my_test_5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0msgd_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0msgd_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    746\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 748\u001b[1;33m                          sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    749\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         X, y = check_X_y(X, y, 'csr', dtype=np.float64, order=\"C\",\n\u001b[1;32m--> 570\u001b[1;33m                          accept_large_sparse=False)\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    757\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[28.4 24.8 26.6 31.9 20.5 25.6 25.6 22.4 28.7 26.9 26.9 35.7 22.  30.5\n 24.9 26.1 22.9 32.8 32.7 26.9 26.1 34.5 27.4 27.6 24.4 28.5 29.2 25.\n 27.9 32.4 20.2 29.5 29.1 31.  26.4 33.5 26.6 27.7 17.7 25.5 32.  29.\n 26.9 25.4 19.6 30.  31.2 36.5 30.4 29.  26.3 25.7 29.2 29.  30.7 25.\n 28.5 29.7 26.3 27.6 25.4 29.4 27.4 22.3 23.9 19.2 26.2 21.  28.2 32.7\n 23.3 30.5 27.7 35.4 31.3 25.1 25.1 22.8 27.6 24.8 27.2 30.8 23.3 17.5\n 34.8 28.5 21.3 35.8 30.4 27.2 23.3 26.3 29.3 27.6 30.8 27.2 26.2 34.\n 29.4 25.6 21.8 22.6 21.6 22.9 27.2 23.8 24.6 20.2 24.3 16.1 23.3 26.3\n 19.2 27.2 23.5 35.8 34.  35.6 17.9 30.  21.7 34.  28.6 27.4 27.4 25.8\n 24.6 26.1 23.9 19.8 32.2 20.6 29.6 27.8 31.6 29.  18.1 24.1 18.4 19.3\n 32.7 19.9 30.  28.8 26.2 31.4 34.  26.2 24.1 20.4 28.7 26.4 20.6 24.5\n 26.5 29.3 31.5 25.9 24.4 19.9 20.5 27.9 30.5 28.7 26.2 30.6 26.4 24.\n 32.5 28.2 28.4 34.8 34.2 23.9 26.6 29.5 28.4 35.6 24.9 28.5 28.8 31.6\n 32.8 18.9 29.4 25.3 18.  21.8 21.5 22.8 26.  27.1 25.8 24.9 25.6 27.7\n 28.7 28.  29.  26.1 26.9 31.8 29.3 21.9 27.  21.2 33.3 35.5 27.2 27.3\n 28.  25.9 22.1 27.5 27.1 30.3 26.8 16.1 22.9 23.8 28.  31.  28.7 29.\n 25.8 29.8 27.6 19.2 26.7 24.8 23.2 32.  22.9 29.2 23.7 26.7 27.1 26.1\n 24.7 21.4 35.3 30.8 25.5 30.7 20.  27.9 23.4 29.5 25.7 33.7 22.4 28.1\n 30.3 28.3 17.4 18.7 18.4 22.1 24.4 23.7 27.4 29.6 22.5 18.  32.8 24.8\n 27.3 29.3 24.9 33.5].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from libitmal import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import matplotlib\n",
    "\n",
    "def transform_commas(line):\n",
    "    retval = ''\n",
    "    inQuote = False\n",
    "    for char in line:\n",
    "        if char == '\"':\n",
    "            inQuote = not inQuote\n",
    "        if inQuote == False and char == ',':\n",
    "            retval += ';'\n",
    "        else:\n",
    "            retval += str(char)\n",
    "    return retval\n",
    "\n",
    "def to_float(x):\n",
    "    if x:\n",
    "        return float(x.strip('\"').replace(',','.'))\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def to_int(x):\n",
    "    if x:\n",
    "        return int(x.strip('\"').replace('.',''))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_beer_consumption_data():\n",
    "    f = open(\"../datasets/beer_consumption/Consumo_cerveja.csv\", \"r\")\n",
    "    replaced = (transform_commas(line) for line in f)\n",
    "    rawData = numpy.loadtxt(replaced,delimiter=';', skiprows=0, dtype=str)\n",
    "    # features = rawData[0]\n",
    "    data = rawData[1:]\n",
    "    max_temp_data = [to_float(x) for x in data[:,3]]\n",
    "    liters_data = [to_int(x) for x in data[:,6]]\n",
    "    return max_temp_data, liters_data\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "target = 92;\n",
    "X,y = get_beer_consumption_data()\n",
    "print(np.asarray(X).shape)\n",
    "print(np.asarray(y).shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.asarray(X),np.asarray(y))\n",
    "print(y_train.shape)\n",
    "print(X_train.shape)\n",
    "X_train = np.asarray([n for n in X_train if n is not None])\n",
    "X_test = np.asarray([n for n in X_test if n is not None])\n",
    "y_train = np.asarray([n for n in y_train if n is not None])\n",
    "y_test = np.asarray([n for n in y_test if n is not None])\n",
    "y_train_5 = (y_train > target)\n",
    "y_test_5 = (y_test > target)\n",
    "sgd_clf = SGDClassifier(max_iter=5, tol=None, random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train_5)\n",
    "print(sgd_clf.predict([[39, 100]]))\n",
    "y_pred_true = cross_val_predict(sgd_clf, X_test, y_test_5, cv=3)\n",
    "beerSGD = confusion_matrix(y_pred_true, y_test_5)\n",
    "print(beerSGD)\n",
    "plt.matshow(beerSGD, cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
