{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "#!/opt/anaconda3/bin/python\n",
    "# https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from csv import reader\n",
    "from math import exp\n",
    "from random import randrange, shuffle\n",
    "from libitmal import utils_v2 as itmalutils\n",
    "\n",
    "def load_csv_data(filename,shuffle=True,verbose=False):\n",
    "    # Load a CSV file\n",
    "    def load_csv(filename):\n",
    "        dataset = list()\n",
    "        with open(filename, 'r') as file:\n",
    "            csv_reader = reader(file)\n",
    "            for row in csv_reader:\n",
    "                if not row:\n",
    "                    continue\n",
    "                dataset.append(row)\n",
    "        return dataset\n",
    "\n",
    "    # Convert string column to float\n",
    "    def str_column_to_float(dataset, column):\n",
    "        for row in dataset:\n",
    "            #print(\"row=\",row,\", column=\",column)    \n",
    "            row[column] = float(row[column].strip())\n",
    "            itmalutils.CheckFloat(row[column])\n",
    "\n",
    "    # Convert string column to integer\n",
    "    def str_column_to_int(dataset, column):\n",
    "        class_values = [row[column] for row in dataset]\n",
    "        unique = set(class_values)\n",
    "        lookup = dict()\n",
    "        for i, value in enumerate(unique):\n",
    "            lookup[value] = i\n",
    "        for row in dataset:\n",
    "            row[column] = lookup[row[column]]\n",
    "        return lookup\n",
    "\n",
    "    # Find the min and max values for each column\n",
    "    def dataset_minmax(dataset):\n",
    "        minmax = list()\n",
    "        stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "        return stats\n",
    "\n",
    "    # Rescale dataset columns to the range 0-1\n",
    "    def normalize_dataset(dataset, minmax):\n",
    "        for row in dataset:\n",
    "            for i in range(len(row)-1):\n",
    "                row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "    # load and prepare data\n",
    "    dataset = load_csv(filename)\n",
    "    for i in range(len(dataset[0])-1):\n",
    "        str_column_to_float(dataset, i)\n",
    "        \n",
    "    # convert class column to integers\n",
    "    str_column_to_int(dataset, len(dataset[0])-1)\n",
    "    # normalize input variables\n",
    "    minmax = dataset_minmax(dataset)\n",
    "    normalize_dataset(dataset, minmax)\n",
    "    if shuffle:\n",
    "        dataset = shuffle_dataset(dataset)\n",
    "    \n",
    "    for i in dataset:\n",
    "        for j in i:\n",
    "            itmalutils.CheckFloat(j,checkrange=True,xmin=0,xmax=1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"type(dataset)=\",type(dataset),\", dataset shape=\",[len(dataset), len(dataset[0])])\n",
    "        print(\"dataset[0]=\", itmalutils.ListToVector(dataset[0]))\n",
    "        print(\"dataset[1]=\", itmalutils.ListToVector(dataset[1]))\n",
    "        print(\"dataset[-1]=\",itmalutils.ListToVector(dataset[-1]))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def cross_validate_evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    print(\"cross_validate_evaluate_algorithm()...\")\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = list()\n",
    "    k=1\n",
    "    for fold in folds:\n",
    "        print(f\"  k-fold={k}/{len(folds)}\")\n",
    "        k += 1\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = list()\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None\n",
    "            \n",
    "        predicted = algorithm(train_set, test_set, *args)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        \n",
    "        accuracy = accuracy_metric(actual, predicted)\n",
    "        scores.append(accuracy) \n",
    "        print(\"    accuracy=\",accuracy)\n",
    "    return scores\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK, NN ready for use...\n"
     ]
    }
   ],
   "source": [
    "# A Neural Network in Python, three layer MLP with activationfunction and BProp\n",
    "\n",
    "# NOTE: transfer() and transfer_derivative() defined here, on outer level, to allow for later modification\n",
    "# Transfer neuron activation, \n",
    "def transfer(z):\n",
    "    # a(z) = 1/(1+exp(-z))\n",
    "    return 1.0 / (1.0 + exp(-z)) \n",
    "\n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    # for a(z) = 1/(1+exp(-z))\n",
    "    #  a'(z) = d(a(z)) / dz = exp(-z)/ ((1+exp(-z)+1)^2) \n",
    "    #        = a(z)*(1-a(z)) \n",
    "    # NOTE: no need to recalc anything, just use a(z)/output to \n",
    "    #       fast find the deriverty!\n",
    "    # [https://en.wikipedia.org/wiki/Backpropagation]\n",
    "    #\n",
    "    return output * (1.0 - output) \n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    \n",
    "    # Calculate neuron activation for an input\n",
    "    def activate(weights, inputs):\n",
    "        activation = weights[-1]\n",
    "        for i in range(len(weights)-1):\n",
    "            activation += weights[i] * inputs[i]\n",
    "        return activation\n",
    "\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    \n",
    "    # Backpropagate error and store in neurons\n",
    "    def backward_propagate_error(network, expected):\n",
    "        for i in reversed(range(len(network))):\n",
    "            layer = network[i]\n",
    "            errors = list()\n",
    "            if i != len(network)-1:\n",
    "                for j in range(len(layer)):\n",
    "                    error = 0.0\n",
    "                    for neuron in network[i + 1]:\n",
    "                        error += (neuron['weights'][j] * neuron['delta'])\n",
    "                    errors.append(error)\n",
    "            else:\n",
    "                for j in range(len(layer)):\n",
    "                    neuron = layer[j]\n",
    "                    errors.append(expected[j] - neuron['output'])\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "                \n",
    "    # Update network weights with error\n",
    "    def update_weights(network, row, l_rate):\n",
    "        assert(l_rate >= 0)\n",
    "        for i in range(len(network)):\n",
    "            inputs = row[:-1]\n",
    "            if i != 0:\n",
    "                inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "            for neuron in network[i]:\n",
    "                for j in range(len(inputs)):\n",
    "                    neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "                neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "    \n",
    "    #print(\"  shape of train=\",[len(train),len(train[0])])\n",
    "    assert l_rate>0\n",
    "    assert n_epoch>0\n",
    "    assert len(train)>0\n",
    "    assert len(train[0])>0\n",
    "    assert n_outputs>0\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            # NOTE: the following is in effect a to_categorical() fun\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1 \n",
    "            #print(\"expected=\",expected)\n",
    "            \n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "\n",
    "# Make a prediction with a network for a single row\n",
    "def predict_row(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))\n",
    "\n",
    "# Make a prediction with a network for a full dataset\n",
    "def predict_network(network, test):\n",
    "    predictions = list()\n",
    "    for row in test:\n",
    "        prediction = predict_row(network, row)\n",
    "        predictions.append(prediction)\n",
    "    return(predictions)\n",
    "\n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    #print(\"    init: n_inputs=\",n_inputs,\", n_hidden=\",n_hidden,\", n_outputs=\",n_outputs)\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random.random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random.random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "# Backpropagation Algorithm With Stochastic Gradient Descent\n",
    "def train_predict_backprop(train, test, l_rate, n_epoch, n_hidden): \n",
    "    n_inputs = len(train[0]) - 1\n",
    "    n_outputs = len(set([row[-1] for row in train]))\n",
    "    print(\"    back_propagation: n_inputs=\",n_inputs,\", n_hidden=\",n_hidden,\" n_outputs=\",n_outputs,\", shapes(train,test)=\",[len(train),len(train[0])],[len(test),len(test[0])])\n",
    "    network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "    \n",
    "    train_network(network, train, l_rate, n_epoch, n_outputs)\n",
    "    return predict_network(network, test)\n",
    "\n",
    "print(\"OK, NN ready for use...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_validate_evaluate_algorithm()...\n",
      "  k-fold=1/5\n",
      "    back_propagation: n_inputs= 7 , n_hidden= 5  n_outputs= 3 , shapes(train,test)= [156, 8] [39, 8]\n",
      "    accuracy= 0.9743589743589743\n",
      "  k-fold=2/5\n",
      "    back_propagation: n_inputs= 7 , n_hidden= 5  n_outputs= 3 , shapes(train,test)= [156, 8] [39, 8]\n",
      "    accuracy= 0.9230769230769231\n",
      "  k-fold=3/5\n",
      "    back_propagation: n_inputs= 7 , n_hidden= 5  n_outputs= 3 , shapes(train,test)= [156, 8] [39, 8]\n",
      "    accuracy= 0.9743589743589743\n",
      "  k-fold=4/5\n",
      "    back_propagation: n_inputs= 7 , n_hidden= 5  n_outputs= 3 , shapes(train,test)= [156, 8] [39, 8]\n",
      "    accuracy= 0.9743589743589743\n",
      "  k-fold=5/5\n",
      "    back_propagation: n_inputs= 7 , n_hidden= 5  n_outputs= 3 , shapes(train,test)= [156, 8] [39, 8]\n",
      "    accuracy= 0.9487179487179487\n",
      "Scores:  [0.9743589743589743, 0.9230769230769231, 0.9743589743589743, 0.9743589743589743, 0.9487179487179487]\n",
      "Mean Accuracy: 0.959\n",
      "  DEBUG: e0= True , e1= False\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# reset random\n",
    "itmalutils.ResetRandom()\n",
    "\n",
    "# Test Backprop on Seeds dataset\n",
    "dataset = load_csv_data('Dat/seeds_dataset.csv',shuffle=True,verbose=False)\n",
    "\n",
    "# evaluate algorithm\n",
    "n_folds  = 5\n",
    "l_rate   = 0.3\n",
    "n_epoch  = 500\n",
    "n_hidden = 5\n",
    "\n",
    "scores = cross_validate_evaluate_algorithm(dataset, train_predict_backprop, n_folds, l_rate, n_epoch, n_hidden)\n",
    "score_mean = sum(scores)/float(len(scores))\n",
    "\n",
    "print(\"Scores: \",scores)\n",
    "print(f\"Mean Accuracy: {score_mean:1.3f}\")\n",
    "\n",
    "# Test, flippes btw. diff expected, values, fixed(?) via itmalutils.ResetRandom() fun\n",
    "expected_score_mean=[.9589743589743588,.9538461538261537] # NOTE: random seems to flip between these, even seed is used!\n",
    "                         \n",
    "e0=itmalutils.InRange(score_mean, expected_score_mean[0]) \n",
    "e1=itmalutils.InRange(score_mean, expected_score_mean[1])\n",
    "print(\"  DEBUG: e0=\",e0,\", e1=\",e1)\n",
    "assert e0 or e1\n",
    "assert (e0 and not e1) or (not e0 and e1)\n",
    "\n",
    "#expected_scores_x0=[ .9743589743589743, .9230769230769231,  .9487179487179487, .9743589743589743, .9487179487179487]\n",
    "#expected_scores_x1=[ .9743589743589743, .9230769230769231,  .9743589743589743, .9743589743589743, .9487179487179487]\n",
    "#itmalutils.AssertInRange(sorted(scores), sorted(expected_scores_x))\n",
    "\n",
    "#expected_scores_0=[.923076923076923,  .9743589743589743, 1.0,                .9487179487179486, .9487179487179486]\n",
    "#expected_scores_1=[.9743589743589743, .923076923076923 ,  .9487179487179486, .9743589743589743, .9487179487179486]\n",
    "#if e0:\n",
    "#    itmalutils.AssertInRange(sorted(scores), sorted(expected_scores_0))\n",
    "#else:\n",
    "#    itmalutils.AssertInRange(sorted(scores), sorted(expected_scores_1))\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape= (199, 7) , y.shape= (199,) , d= 7 , num_classes= 3\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "def FromCategorical(predict_c, y):\n",
    "    assert predict_c.ndim==2 and predict_c.shape[1]>1\n",
    "    predicted = np.argmax(predict_c, axis=1)\n",
    "    assert y.shape==predicted.shape\n",
    "    return predicted\n",
    "\n",
    "X, y = itmalutils.DToXy(dataset)\n",
    "\n",
    "num_classes = 3\n",
    "n=X.shape[0]\n",
    "d=X.shape[1]\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_c = keras.utils.to_categorical(y, num_classes)\n",
    "assert (np.argmax(y_c, axis=1)==y).all()\n",
    "\n",
    "print(\"X.shape=\",X.shape,\", y.shape=\",y.shape,\", d=\",d,\", num_classes=\",num_classes)\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myMLPClassifier:\n",
      "  fitted  =False\n",
      "  l_rate  =0.3\n",
      "  n_epoch =500\n",
      "  input   =7\n",
      "  layer[0]=5\n",
      "    w(5)=\n",
      "      w[0]:     0.13     0.85     0.76     0.26     0.50     0.45     0.65     0.79 \n",
      "      w[1]:     0.09     0.03     0.84     0.43     0.76     0.00     0.45     0.72 \n",
      "      w[2]:     0.23     0.95     0.90     0.03     0.03     0.54     0.94     0.38 \n",
      "      w[3]:     0.22     0.42     0.03     0.22     0.44     0.50     0.23     0.23 \n",
      "      w[4]:     0.22     0.46     0.29     0.02     0.84     0.56     0.64     0.19 \n",
      "  layer[1]=3\n",
      "    w(3)=\n",
      "      w[0]:     0.99     0.86     0.12     0.33     0.72     0.71 \n",
      "      w[1]:     0.94     0.42     0.83     0.67     0.30     0.59 \n",
      "      w[2]:     0.88     0.85     0.51     0.59     0.03     0.24 \n",
      "do train0\n",
      "do train1\n",
      "Score(/accuracy): 0.975\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "x=0.9748743718592965 is not within the range [0.984;0.986] for eps=0.001, got eps=0.010125628140703502",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-514f14aad1e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mexpected_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.985\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mitmalutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAssertInRange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1E-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ASE/ML/itmal/libitmal/utils_v2.py\u001b[0m in \u001b[0;36mAssertInRange\u001b[0;34m(x, e, eps, verbose)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"InRange(x=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",e=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\",eps=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\") x in [\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"]: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mok\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\"x=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" is not within the range [\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"] for eps=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\", got eps=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabsdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mInRange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1E-9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: x=0.9748743718592965 is not within the range [0.984;0.986] for eps=0.001, got eps=0.010125628140703502"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class myMLPClassifier(BaseEstimator):\n",
    "    def __init__(self, inputs=7, hidden=5, outputs=3, l_rate=0.3, n_epoch=500):\n",
    "        self.n_inputs = inputs\n",
    "        self.n_hidden = hidden\n",
    "        self.n_outputs= outputs\n",
    "        self.l_rate   = l_rate \n",
    "        self.n_epoch  = n_epoch\n",
    "        self.fitted=False\n",
    "        \n",
    "        self.network=initialize_network(self.n_inputs, self.n_hidden, self.n_outputs)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        assert X.shape[1]==self.n_inputs\n",
    "        assert y.ndim==1, \"cannot handle multiclass/categorical y\"\n",
    "        \n",
    "        print(\"do train0\")\n",
    "        # NOTE: convert X,y matrix/vector to dataset\n",
    "        train = itmalutils.XyToD(X, y)     \n",
    "        print(\"do train1\")\n",
    "        train_network(self.network, train, self.l_rate, self.n_epoch, self.n_outputs)\n",
    "        self.fitted=True \n",
    "        \n",
    "    def predict(self, X):\n",
    "        assert self.fitted\n",
    "        assert X.shape[1]==self.n_inputs\n",
    "        \n",
    "        # NOTE: pass X as np.array instead of list...works!\n",
    "        predictions = predict_network(self.network, X)\n",
    "        return itmalutils.ListToVector(predictions)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        assert y.ndim==1, \"cannot handle multiclass/categorical y\"\n",
    "\n",
    "        y_pred=self.predict(X)\n",
    "        assert y_pred.shape==y.shape\n",
    "        acc = accuracy_metric(y, y_pred) \n",
    "        return acc\n",
    "\n",
    "    #def evaluate(self, X, y):\n",
    "    #    return score(self, X, y)\n",
    "    \n",
    "    def __str__ (self, verbose=False):\n",
    "        s = \"myMLPClassifier:\"\n",
    "        s += \"\\n  fitted  =\" + str(self.fitted)\n",
    "        s += \"\\n  l_rate  =\" + str(self.l_rate)\n",
    "        s += \"\\n  n_epoch =\" + str(self.n_epoch)\n",
    "        s += \"\\n  input   =\" + str(self.n_inputs)\n",
    "        n=len(self.network)\n",
    "        for i in range(n):\n",
    "            s += \"\\n  layer[\" + str(i) + \"]=\" + str(len(self.network[i]))\n",
    "            if verbose:\n",
    "                w = self.network[i]\n",
    "                m = len(w)\n",
    "                s += \"\\n    w(\" + str(m) + \")=\"\n",
    "                for j in range(m):\n",
    "                    ww = w[j]['weights']\n",
    "                    s += \"\\n      w[\" + str(j) + \"]: \"\n",
    "                    for k in range(len(ww)):\n",
    "                        s += \"%8.2f \" % ww[k]\n",
    "        return s\n",
    "    \n",
    "itmalutils.ResetRandom()\n",
    "\n",
    "m = myMLPClassifier()\n",
    "print(m.__str__(True))\n",
    "\n",
    "m.fit(X, y)\n",
    "p=m.predict(X)\n",
    "score=m.score(X, y)\n",
    "\n",
    "print(f\"Score(/accuracy): {score:1.3f}\")\n",
    "\n",
    "expected_score=0.985\n",
    "itmalutils.AssertInRange(score, expected_score, eps=1E-3)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: CEF, unfinished from here!\n",
    "\n",
    "print(X)\n",
    "#sdf\n",
    "\n",
    "print(y)\n",
    "def myargmax(p):\n",
    "    assert p.ndim==2\n",
    "    n=p.shape[0]\n",
    "    d=p.shape[1]\n",
    "    am=np.empty([n])\n",
    "    \n",
    "    for i in range(n):\n",
    "        m=p[i,0]\n",
    "        mj=0\n",
    "        for j in range(d):\n",
    "            #print(\"i=\",i,\", j=\",j,\", p[i,j]=\",p[i,j])\n",
    "            if p[i,j]>m:\n",
    "                m=p[i,j]\n",
    "                mj=j\n",
    "        #print(mj)\n",
    "        am[i]=mj\n",
    "    return am\n",
    "#[[0.6713763  0.40817234 0.46002817]\n",
    "# [0.68884546 0.4148609  0.4603176 ]\n",
    "# [0.7057993  0.41240785 0.46892616]\n",
    "# [0.6829971  0.40489745 0.4576759 ]\n",
    "# [0.6907633  0.41742143 0.46148202] ...\n",
    "print(myargmax(p))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libitmal import dataloaders_v3 as dataloaders\n",
    "\n",
    "X, y = dataloaders.MNIST_GetDataSet(fetchmode=False)\n",
    "\n",
    "X = X.reshape(70000, 784)\n",
    "X = X/np.float32(255) # NOTE: remembered convert to float and scale \n",
    "\n",
    "#y = (y == 5)    \n",
    "\n",
    "print(f\"X.shape={X.shape}\")\n",
    "print(f\"  type(X[0][0])={type(X[0][0])}\")\n",
    "print(f\"  X.dtype={X.dtype}\")\n",
    "print(f\"  np.max(X)={np.max(X)}\")\n",
    "print(f\"  np.min(X)={np.min(X)}\")\n",
    "print(f\"y.shape={y.shape}\")\n",
    "print(f\"  np.max(y)={np.max(y)}\")\n",
    "print(f\"  np.min(y)={np.min(y)}\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "m = myMLPClassifier(inputs=28*28, hidden=12, outputs=2)\n",
    "\n",
    "m.fit(X, y)\n",
    "p=m.predict(X)\n",
    "score=m.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def RunCV(cvmodel, expected_score_mean, verbose=False):\n",
    "    itmalutils.ResetRandom()\n",
    "\n",
    "    print(\"CV: \",cvmodel.__str__(verbose))\n",
    "    scores = cross_val_score(cvmodel, X, y, cv=n_folds)\n",
    "    score_mean = scores.mean()\n",
    "\n",
    "    print(\"  scores:\",scores)\n",
    "    print(f\"  mean accuracy: {score_mean:1.3f}\")\n",
    "\n",
    "    if not itmalutils.InRange(score_mean, expected_score_mean):\n",
    "        print(f\"WARNING: not in range, score_mean={score_mean} and expected_score_mean={expected_score_mean}\")\n",
    "    else:\n",
    "        print(\"CV: DONE\")\n",
    "\n",
    "RunCV(m, 0.9648717948717949, True) # was: 0.9597435897435898\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import tanh, cos, acos, sin\n",
    "from enum import Enum\n",
    "\n",
    "class ActivationType(Enum):\n",
    "    NONE   =-1\n",
    "    SIGMOID=0 # or LOGIT\n",
    "    RELU   =1\n",
    "    TANH   =2\n",
    "    COS    =3\n",
    "    \n",
    "g_actitype=ActivationType.NONE\n",
    "\n",
    "def transfer(z):\n",
    "    #t = str(type(activation))\n",
    "    #assert t==\"<class 'numpy.float64'>\" or t==\"<class 'float'>\"\n",
    "    #itmalutils.CheckFloat(activation)\n",
    "        \n",
    "    if g_actitype==ActivationType.SIGMOID:\n",
    "        return 1.0 / (1.0 + exp(-z))\n",
    "    elif g_actitype==ActivationType.RELU:\n",
    "        return np.maximum(0,z)\n",
    "    elif g_actitype==ActivationType.TANH:\n",
    "        return tanh(z)\n",
    "    elif g_actitype==ActivationType.COS:\n",
    "        return cos(z)\n",
    "    else:\n",
    "        assert False, \"wrong g_actitype value\"\n",
    "\n",
    "def transfer_derivative(output):\n",
    "    #t = str(type(output))\n",
    "    #assert t==\"<class 'numpy.float64'>\" or t==\"<class 'float'>\"\n",
    "    #itmalutils.CheckFloat(output)\n",
    "    \n",
    "    if g_actitype==ActivationType.SIGMOID:\n",
    "        return output * (1.0 - output)\n",
    "    elif g_actitype==ActivationType.RELU:\n",
    "        if output>0.0:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    elif g_actitype==ActivationType.TANH:\n",
    "        # d(tanh(z))/dz = 1 - (tanh(z))^2\n",
    "        return 1 - output*output\n",
    "    elif g_actitype==ActivationType.COS:\n",
    "        a=acos(output)\n",
    "        return -sin(a)\n",
    "    else:\n",
    "        assert False, \"wrong g_actitype value\"\n",
    "\n",
    "def PrintTransferAndDerivate(a=[-1, -0.1, 0, 1, 2, 10]):\n",
    "    print(\"g_actitype=\",g_actitype) \n",
    "    for i in a:\n",
    "        output=transfer(i)\n",
    "        dev=transfer_derivative(output)\n",
    "        print(f\"  transfer           (z={i:6.3f}) = {output:6.3f}\")\n",
    "        print(f\"  transfer_derivative(z={i:6.3f}) = {dev:6.3f}\")\n",
    "    print(\"\")\n",
    "    \n",
    "print(\"OK, new transfer funs ready for use..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_actitype = ActivationType.RELU \n",
    "\n",
    "print(\"Parameter search: l_rate=.. , g_actitype=\",g_actitype)\n",
    "PrintTransferAndDerivate()\n",
    "\n",
    "m = myMLPClassifier(l_rate=0.2, n_epoch=500)    \n",
    "RunCV(m, 0.3366666666666666)\n",
    "\n",
    "m = myMLPClassifier(l_rate=0.1, n_epoch=500)    \n",
    "RunCV(m, 0.3266666666666666)\n",
    "\n",
    "m = myMLPClassifier(l_rate=0.05, n_epoch=500)    \n",
    "RunCV(m, 0.31166666666666665)\n",
    "\n",
    "m = myMLPClassifier(l_rate=0.01, n_epoch=500)    \n",
    "RunCV(m, 0.9650000000000001)\n",
    "\n",
    "m = myMLPClassifier(l_rate=0.005, n_epoch=500)    \n",
    "RunCV(m, 0.9698717948717949)\n",
    "\n",
    "m = myMLPClassifier(l_rate=0.001, n_epoch=500)    \n",
    "RunCV(m, 0.9346153846153845)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rate_best=0.005\n",
    "\n",
    "print(\"Parameter search: l_rate=\",l_rate,\", n_epoch=.. , g_actitype=\",g_actitype)\n",
    "\n",
    "m = myMLPClassifier(l_rate=l_rate_best, n_epoch=1)    \n",
    "RunCV(m, 0.4625641025641025)\n",
    "\n",
    "m = myMLPClassifier(l_rate=l_rate_best, n_epoch=2)    \n",
    "RunCV(m, 0.5975641025641025)\n",
    "\n",
    "m = myMLPClassifier(l_rate=l_rate_best, n_epoch=5)    \n",
    "RunCV(m, 0.7584615384615384)\n",
    "\n",
    "m = myMLPClassifier(l_rate=l_rate_best, n_epoch=10)    \n",
    "RunCV(m, 0.8742307692307693)\n",
    "\n",
    "m = myMLPClassifier(l_rate=l_rate_best, n_epoch=50)    \n",
    "RunCV(m, 0.9194871794871796)\n",
    "\n",
    "m = myMLPClassifier(l_rate=l_rate_best, n_epoch=100)    \n",
    "RunCV(m, 0.9346153846153846)\n",
    "\n",
    "m = myMLPClassifier(l_rate=l_rate_best, n_epoch=200)    \n",
    "RunCV(m, 0.9697435897435897)\n",
    "\n",
    "m = myMLPClassifier(l_rate=l_rate_best, n_epoch=500)    \n",
    "RunCV(m, 0.9698717948717949)\n",
    "\n",
    "m = myMLPClassifier(l_rate=l_rate_best, n_epoch=1000)    \n",
    "RunCV(m, 0.9648717948717949)\n",
    "\n",
    "# NOTE: takes too long\n",
    "#m = myMLPClassifier(l_rate=l_rate_best, n_epoch=10000)    \n",
    "#RunCV(m, 0.8894871794871795)\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_scores=[0.7735897435897436, 0.9698717948717949, 0.9650000000000001, 0.5971794871794872]\n",
    "\n",
    "k=0\n",
    "for i in [ActivationType.SIGMOID, ActivationType.RELU, ActivationType.TANH, ActivationType.COS]:\n",
    "    g_actitype = i \n",
    "    PrintTransferAndDerivate()\n",
    "\n",
    "    n_epoch_best=500\n",
    "    l_rate_best=0.005\n",
    "    m = myMLPClassifier(l_rate_best, n_epoch_best)    \n",
    "    RunCV(m, expected_scores[k]) # NOTE: ++k NOT c/c++ like!\n",
    "    k += 1\n",
    "    print(\"\")\n",
    "\n",
    "print(\"OK\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
